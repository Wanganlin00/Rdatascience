# factor-forcats包

```{r}
              #basic####

x1 <- c("Dec", "Apr", "Jan", "Mar")
x2 <- c("Dec", "Apr", "Jam", "Mar")

month_levels <- c(
  "Jan", "Feb", "Mar", "Apr", "May", "Jun", 
  "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"
)
factor(x1, levels = month_levels,labels = c(1:12)) #默认按字母顺序排序
fct(x1)         #按第一次出现排序
fct(x1, levels = month_levels)

levels(x1) #因子水平
```

```{r}


csv <- "
month,value
Jan,12
Feb,56
Mar,12"
df<-read_csv(csv, col_types = cols(month =readr::col_factor(month_levels)))
df$month



                                     # 修改因子顺序 ####
forcats::gss_cat  #因子数据集

relig_summary <- gss_cat |>
  group_by(relig) |>
  summarize(
    tvhours = mean(tvhours, na.rm = TRUE),
    n = n()
  )
ggplot(relig_summary, aes(x = tvhours, y = relig,size=n)) + geom_point()
ggplot(relig_summary,aes(x = tvhours,     #按tvhours升序 重排无序因子水平 
                         y = fct_reorder(relig,tvhours),size=n)) +
  geom_point()


rincome_summary <- gss_cat |>
  group_by(rincome) |>
  summarize(
    age = mean(age, na.rm = TRUE),
    n = n()
  )
rincome_summary
ggplot(rincome_summary,aes(x=age,y=rincome))+
  geom_point()
ggplot(rincome_summary, aes(x = age,     #将任意数量的水平前移至第一
                            y = fct_relevel(rincome, c("Not applicable")))) +
  geom_point()

by_age <- gss_cat |>
  dplyr::filter(!is.na(age)) |> 
  count(age, marital) |>
  group_by(age) |>
  mutate(
    prop = n / sum(n)
  )
by_age
ggplot(by_age, aes(x = age, y = prop, color = marital)) +
  geom_line(linewidth = 1) + 
  scale_color_brewer(palette = "Set1")
                                          
ggplot(by_age, aes(x = age, y = prop,  #按与最大值关联的值对因子重新排序
                   color = fct_reorder2(marital, age, prop))) +
  geom_line(linewidth = 1) +
  scale_color_brewer(palette = "Set1") + 
  labs(color = "marital") 


gss_cat |>                    #单独fct_infreq()降序，合用升序
  mutate(marital = marital |> fct_infreq() |> fct_rev()) |>
  ggplot(aes(x = marital)) +
  geom_bar()






                            #修改因子名####
gss_cat |> count(partyid)

gss_cat |>
  mutate(
    partyid = fct_recode(partyid,  #重编因子名
                         "Republican, strong"    = "Strong republican",
                         "Republican, weak"      = "Not str republican",
                         "Independent, near rep" = "Ind,near rep",
                         "Independent, near dem" = "Ind,near dem",
                         "Democrat, weak"        = "Not str democrat",
                         "Democrat, strong"      = "Strong democrat"
                        # "Other"                 = "No answer",
                        # "Other"                 = "Don't know",
                        # "Other"                 = "Other party"
    )
  ) |>
  count(partyid)

gss_cat |>
  mutate(
    partyid = fct_collapse(partyid,  #合并水平
                           "other" = c("No answer", "Don't know", "Other party"),
                           "rep" = c("Strong republican", "Not str republican"),
                           "ind" = c("Ind,near rep", "Independent", "Ind,near dem"),
                           "dem" = c("Not str democrat", "Strong democrat")
    )
  ) |>
  count(partyid)


relig_summary 
gss_cat |>
  mutate(relig = fct_lump_lowfreq(relig)) |> #合并除最大组以外的水平为Other
  count(relig)

gss_cat |>
  mutate(relig = fct_lump_n(relig, n = 3)) |> #合并较小组水平为第n+1组Other
  count(relig, sort = TRUE)  #排序

gss_cat |>
  mutate(relig = fct_lump_min(relig,min=100)) |> #合并出现min次以下的水平为Other
  count(relig)

relig_summary |> 
  mutate(
    prop=n/sum(n)
  )
gss_cat |>
  mutate(relig = fct_lump_prop(relig,prop=0.1)) |> #合并出现prop以下的水平为Other
  count(relig)
gss_cat |>
  mutate(relig = fct_lump_prop(relig,prop=-0.1)) |> #合并出现-prop以上的水平为Other
  count(relig)


                               #有序因子####
```

# hierarchical——data

```{r}
@ -1,175 +0,0 @@
library(tidyverse)
library(repurrrsive)
library(jsonlite)
#rectangling
##来自JSON或XML的嵌套数据
#JSON 是 javascript object notation 的缩写


                                                  #  list  ####       

list(a = 1:2, b = 1:3, c = 1:4)
c(c(1, 2), c(3, 4))            #一维

x3<-c(list(1, 2), list(3, 4))  #多个成分（向量、矩阵、数组、数据框）组成list
str(x3)
list(a=c(1),b=2,c=3,d=4)

#Hierarchy 层次结构 \树状结构
x4<- list(list(1, 2), list(3, 4))         
str(x4)
x5 <- list(x1=1,x2=list(y1=2,y2=list(z1=3, z2=list(p1=4, p2=list(q1=5)))))
x5
str(x5)
View(x5)

#list-columns 列表列
df <- tibble(
  x = 1:2, 
  y = c("a", "b"),
  z = list(list(1, 2), list(3, 4, 5))
)
df

                          #Unnesting 取消嵌套####

#unnest_wider()
df1 <- tribble(
  ~x, ~y,
  1, list(a = 11, b = 12),
  2, list(a = 21, b = 22),
  3, list(a = 31, b = 32),
)
df1
tidyr::unnest_wider(df1,y)          #子列表已命名->列
tidyr::unnest_wider(df1,y,names_sep = "_")          #消除重复名称

#unnest_longer() 
df2 <- tribble(
  ~x, ~y,
  1, list(11, 12, 13),
  2, list(21),
  3, list(31, 32),
)
df2
tidyr::unnest_longer(df2,y)        #子列表未命名->行

df6 <- tribble(
  ~x, ~y,
  "a", list(1, 2),
  "b", list(3),
  "c", list()
)
df6 |> unnest_longer(y,keep_empty = TRUE) #保留空行

#不一致的类型
df4 <- tribble(
  ~x, ~y,
  "a", list(1),
  "b", list("a", TRUE, 5)
)
df4 |> unnest_longer(y)  #子列表->行


                                # 案例研究   ####
#非常宽的数据
repos <- tibble(json = gh_repos)
repos          # 6行列表
repos|> 
  unnest_longer(json)   # 176行命名子列表
repos |> 
  unnest_longer(json) |> 
  unnest_wider(json)       #  176×68

repos |> 
  unnest_longer(json) |> 
  unnest_wider(json) |> 
  select(id, full_name, owner, description) |> 
  unnest_wider(owner,names_sep = "_")      #  "_" 消除重复名称


#关系数据
chars<-tibble(json=got_chars) 
chars
chars |> 
  unnest_wider(json)

chars |> 
  unnest_wider(json) |> 
  select(name,titles) |> 
  unnest_longer(titles) 

tibble(json=got_chars) %>% 
  hoist(json,'name','titles') %>% #等价
  unnest_longer(titles)


#深度嵌套
gmaps_cities
gmaps_cities |> 
  unnest_wider(json)

gmaps_cities |> 
  unnest_wider(json) |> 
  select(-status) |> 
  unnest_longer(results) 

locations <- gmaps_cities |> 
  unnest_wider(json) |> 
  select(-status) |> 
  unnest_longer(results) |> 
  unnest_wider(results)
locations
locations |> 
  select(city, formatted_address, geometry) |> 
  hoist(
    geometry,
    ne_lat = c("bounds", "northeast", "lat"),  #hoist() 直接提取
    sw_lat = c("bounds", "southwest", "lat"),
    ne_lng = c("bounds", "northeast", "lng"),
    sw_lng = c("bounds", "southwest", "lng"),
  )


                              #JSON格式####
# A path to a json file inside the package:
gh_users_json()

gh_users2 <- read_json(gh_users_json())
# Check it's the same as the data we were using previously
identical(gh_users, gh_users2)

#readr::parse_date()readr::parse_datetime()readr::parse_double()
str(parse_json('1'))
str(parse_json('[1, 2, 3]')) #数组

str(parse_json('{"x": [1, 2, 3],"y":[5,6]}'))#对象


json <- '[
  {"name": "John", "age": 34},
  {"name": "Susan", "age": 27}        
]'                #数组
df <- tibble(json = parse_json(json)) # json是df的一个列向量成分
df
unnest_wider(df,json)

json <- '{
  "status": "OK", 
  "results": [
    {"name": "John", "age": 34},
    {"name": "Susan", "age": 27}
 ]
}
'                #对象
df <- tibble(json = list(parse_json(json)))
df
df |> 
  unnest_wider(json) |> 
  unnest_longer(results) |> 
  unnest_wider(results)


df <- tibble(results = parse_json(json)$results)
df |> 
  unnest_wider(results)
```

# 网络抓取

```{r}
@ -1,110 +0,0 @@
library(tidyverse)
library(rvest)

 #  HTML basic：HyperText Markup Language 超文本标记语言 ####
"<html>                  #document metadata
  <head>                       #start tag <...>
  <title>Page title</title>
  </head>                     #end tag  </...>
<body>   #contents  # Block tags: <h1><section><p><ol> <ul>heading 1,section,paragraph,ordered list
 <h1 id='first' >  A heading</h1>   #<tag attributes> id,class 命名属性 name1='value1'
   <p>Some text &amp; <b>some bold text.</b></p>   #Inline tags:<b><i><a>  bold,italics,link
                                #  HTML escapes： &gt;&lt;&amp    greater than:>,less than:<，ampersand：&
   <a href='webscraping' > https://r4ds.hadley.nz/webscraping </a> #href <a>
   <img src='myimg.png' width='100' height='100'>   #src <img>
</body>
"
                                        #提取数据 ####
read_html("http://rvest.tidyverse.org/")

                       #查找元素
html <- minimal_html("
  <h1>This is a heading</h1>
  <p id='first'>This is a paragraph</p>
  <p id='important'>This is an important paragraph</p>
  <p class='important'>This is an important paragraph</p>
  <p class='first'>This is a  paragraph</p>
   <img id='first',src='myimg.png' width='100' height='100'> 
")
#cascading style sheets:级联样式表     CSS选择器
html
html |> html_elements("p")  #selects all p
html |> html_elements(".first") # all  .class
html |> html_elements("#first")#  all   #id
    
html |> html_element("p")  #返回第一个

html |> html_elements("b")    #> {xml_nodeset (0)}
html |> html_element("b")     #> {xml_missing}   #> <NA>

#嵌套选择 Nesting selections
html <- minimal_html("
  <ul>
    <li><b>C-3PO</b> is a <i>droid</i> that weighs <span class='weight'>167 kg</span></li>
    <li><b>R4-P17</b> is a <i>droid</i></li>
    <li><b>R2-D2</b> is a <i>droid</i> that weighs <span class='weight'>96 kg</span></li>
    <li><b>Yoda</b> weighs <span class='weight'>66 kg</span></li>
  </ul>
  ")
characters <- html |> html_elements("li")
characters
characters |> html_element("b")
characters |> html_element(".weight")
characters |> html_elements(".weight")

# Text 
characters |> 
  html_element("b") |> 
  html_text2()
characters |> 
  html_element(".weight") |> 
  html_text2()                 #提取纯文本内容
#attributes
html <- minimal_html("
  <p><a href='https://en.wikipedia.org/wiki/Cat'>cats</a></p>
  <p><a href='https://en.wikipedia.org/wiki/Dog'>dogs</a></p>
")
html |> 
  html_elements("p") |> 
  html_element("a") |> 
  html_attr("href")

#table              <table><tr><th><td> 行、标题heading、数据

html <- minimal_html("
  <table class='mytable'>
    <tr><th>x</th>   <th>y</th></tr>
    <tr><td>1.5</td> <td>2.7</td></tr>
    <tr><td>4.9</td> <td>1.3</td></tr>
    <tr><td>7.2</td> <td>8.1</td></tr>
  </table>
  ")
html |> 
  html_element(".mytable") |> 
  html_table(convert = F)

                                 #示例####
#1
url <- "https://rvest.tidyverse.org/articles/starwars.html"
html <- read_html(url)

section <- html |> html_elements("section") #提取元素section
section
tibble(
  title = section |> 
    html_element("h2") |>   #提取h2标题文本：电影名称
    html_text2(),
  released = section |> 
    html_element("p") |>    #提取p段落文本：日期
    html_text2() |> 
    str_remove("Released: ") |> 
    parse_date(),
  director = section |>       
    html_element(".director") |> #提取.class=director属性：导演
    html_text2(),
  intro = section |> 
    html_element(".crawl") |>  #提取.class=crawl属性：简介
    html_text2()
)
#2 IMDB top 250 电影

```

# iteration

```{r}

                          #读取多个文件####
rm(list = ls())
# 分部进行  list.files(path,pattern,full.name)
paths <- list.files("data", pattern = "sales[.]csv$", 
                    full.names = TRUE,recursive = TRUE)
paths
library(arrow)
files<-list(
  read_csv("data/01-sales.csv"),
  read_csv("data/02-sales.csv"),
  read_csv("data/03-sales.csv")
)
files

rm(list = ls())
                      #循环迭代  purrr::map(),list_rbind(),list_cbind ####
map(x, f)
list( f(x[[1]]) , f(x[[2]]) , ..., f(x[[n]] ))   #  map(list,function)

files <- map(paths, read_csv) 
length(files)
files

purrr::list_rbind(files)    #行合并


#路径中的数据
set_names(paths,basename)  #从路径中提取文件名,文件名也是一列数据
files <- list(
  "01-sales.csv"=read_csv("data/01-sales.csv"),
  "02-sales.csv"=read_csv("data/02-sales.csv"),
  "03-sales.csv"=read_csv("data/03-sales.csv"))

files <- map(set_names(paths,basename),read_csv)#简写 文件名进入数据框
files
files$`01-sales.csv`

paths |> 
  set_names(basename) |> 
  map(read_csv) |> 
  list_rbind(names_to = "file_name_csv") |> #文件名 to 列名
  write_csv( "data/2019第一季度销售情况.csv") #保存



#推荐多次简单迭代
paths |> 
  map(read_csv) |> 
  map(\(df) df |> dplyr::filter(!is.na(n)))|> 
  map(\(df) df |> mutate(id =LETTERS[1:nrow(df)] )) |> 
  list_rbind()

#异构数据
df_types <- function(df) {  #捕获数据框结构
  tibble(
    col_name = names(df), 
    col_type = map_chr(df, vctrs::vec_ptype_full),
    n_miss = map_int(df, \(x) sum(is.na(x)))
  )
}
files |> 
  map(df_types) |> 
  list_rbind(names_to = "file_name") |> 
  select(-n_miss) |> 
  pivot_wider(names_from = col_name, values_from = col_type)


#处理故障
files <- paths |> 
  map(possibly(\(path) readxl::read_excel(path), NULL))
data <- files |> list_rbind()

failed <- map_vec(files, is.null)  #获取失败文件路径
paths[failed]



                           #保存多个对象####
rm(list = ls())
paths <- list.files("data", pattern = "sales[.]csv$", 
                    full.names = TRUE,recursive = TRUE)

#写入数据库####
con <- DBI::dbConnect(duckdb::duckdb())
duckdb::duckdb_read_csv(con, "sales", paths) #读取到数据库
library(DBI)
library(dbplyr)
dbListTables(con)

con |> 
  dbReadTable("sales") |> 
  as_tibble()
tbl(con, "sales") 

DBI::dbCreateTable(con, "gapminder", template)  #创建数据库table
append_file <- function(path) {       #追加文件路径
  df <- readxl::read_excel(path)
  df$year <- parse_number(basename(path))
  DBI::dbAppendTable(con, "gapminder", df)
}
paths |> map(append_file)
paths |> walk(append_file)  #不看输出

                                       #写出 csv 文件####

by_clarity <- diamonds |> 
  group_nest(clarity)        #分组列表列
by_clarity

by_clarity <- by_clarity |> 
  mutate(path = str_glue("data/diamonds-{clarity}.csv"))#添加文件名
by_clarity
                                  #map2()walk2() 两个参数变化
map2(by_clarity$data, by_clarity$path, write_csv) #控制台有输出
walk2(by_clarity$data, by_clarity$path, write_csv) #控制台无输出


                             #保存绘图####
carat_histogram <- function(df) {
  ggplot(df, aes(x = carat)) + geom_histogram(binwidth = 0.1)  
}
carat_histogram(by_clarity$data[[1]])

by_clarity <- diamonds |> 
  group_nest(clarity) 
by_clarity <- by_clarity |> 
  mutate(
    plot = map(data, carat_histogram),
    path = str_glue("data/histogram-{clarity}.png")
  )
by_clarity
walk2(
  by_clarity$path,
  by_clarity$plot,
  \(path, plot) ggsave(path, plot, width = 6, height = 6)
)
```

# 缺失值

```{r}
@ -1,92 +0,0 @@
#dplyr 
#tidyr

                          #显式缺失值   Explicit missing values   ####

#last observation carried forward    locf
treatment <- tribble(
  ~person,           ~treatment, ~response,
  "Derrick Whitmore", 1,         7,
  NA,                 2,         10,
  NA,                 3,         NA,
  "Katherine Burke",  1,         4
)
treatment |>
  tidyr::fill(everything())  #填充NA上一个观测的值


# Fixed values
x <- c(1, 4, 5, 7, -99)
dplyr::na_if(x,-99)

x <- c(1, 4, 5, 7, NA)
x
dplyr::coalesce(x, 0)

# NaN :not a number
x<-c(0/0,0*Inf,Inf-Inf,1/0)
x
is.nan(x)


                             #隐式缺失值 Implicit missing values  ####
stocks <- tibble(
  year  = c(2020, 2020, 2020, 2020, 2021, 2021, 2021),
  qtr   = c(   1,    2,    3,    4,    2,    3,    4), #缺失2021第一季度
  price = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)      
)
#Pivoting

stocks |>
  pivot_wider(
    names_from = qtr, 
    values_from = price
  )
#Complete
stocks |>
  tidyr::complete(year, qtr)

#Joins
library(nycflights13)

flights |> 
  distinct(faa = dest) |> 
  anti_join(airports)
flights |> 
  distinct(tailnum) |> 
  anti_join(planes)
 
                                          #因子和空组####

health <- tibble(
  name   = c("Ikaia", "Oletta", "Leriah", "Dashay", "Tresaun"),
  smoker = factor(c("no", "no", "no", "no", "no"), levels = c("yes", "no")),
  age    = c(34, 88, 75, 47, 56),
)
health |> count(smoker)
health |> count(smoker, .drop = FALSE) #保留因子所有水平

p<-ggplot(health, aes(x = smoker)) +
  geom_bar() 

p+scale_x_discrete()
p+scale_x_discrete(drop = FALSE) #保留因子所有水平

health |> 
  group_by(smoker, .drop = FALSE) |> #保留因子所有水平
  summarize(
    n = n(),
    mean_age = mean(age),
    min_age = min(age),
    max_age = max(age),
    sd_age = sd(age)
  )
health |> 
  group_by(smoker) |> 
  summarize(
    n = n(),
    mean_age = mean(age),
    min_age = min(age),
    max_age = max(age),
    sd_age = sd(age)
  ) |> 
  complete(smoker)# 显式处理
```
